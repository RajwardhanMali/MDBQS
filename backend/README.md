# MDBQS Backend - Multi-Database Query System

A sophisticated FastAPI-based backend that enables querying multiple heterogeneous databases (SQL, NoSQL, Graph, Vector) through a unified natural language interface powered by Google Gemini AI.

## ğŸ“‹ Overview

**MDBQS** (Multi-Database Query System) is an intelligent query orchestration platform that:

- Accepts natural language queries from users
- Decomposes queries into database-specific execution plans using LLM-assisted planning
- Executes queries in parallel across multiple data sources (PostgreSQL, MongoDB, Neo4j, Milvus)
- Fuses heterogeneous results with provenance tracking
- Provides a schema discovery interface for field-level searching

### Key Features

âœ… **Multi-source query execution** â€” SQL, Document (MongoDB), Graph (Neo4j), Vector (Milvus)  
âœ… **LLM-powered planning** â€” Google Gemini 2.5 Flash for intelligent query decomposition  
âœ… **Parallel execution** â€” Async task orchestration for fast result aggregation  
âœ… **Schema indexing** â€” Searchable field registry across all data sources  
âœ… **Provenance tracking** â€” Trace which source contributed each data element  
âœ… **Model Context Protocol (MCP)** â€” Plugin architecture for data source integration  

---

## ğŸ—ï¸ Project Structure

```
backend/
â”œâ”€ app/
â”‚  â”œâ”€ main.py                        # FastAPI app setup, middleware, router registration
â”‚  â”œâ”€ api/
â”‚  â”‚  â””â”€ v1/
â”‚  â”‚     â”œâ”€ query.py                 # POST /api/v1/query endpoint
â”‚  â”‚     â””â”€ schema.py                # GET /api/v1/schema/search endpoint
â”‚  â”œâ”€ core/
â”‚  â”‚  â”œâ”€ config.py                   # Settings & environment variables
â”‚  â”‚  â””â”€ llm/
â”‚  â”‚     â”œâ”€ gemini_client.py         # Gemini API wrapper
â”‚  â”‚     â””â”€ prompts.py               # Prompt templates
â”‚  â”œâ”€ services/
â”‚  â”‚  â”œâ”€ planner.py                  # Query decomposition (LLM-based)
â”‚  â”‚  â”œâ”€ execution.py                # Parallel execution against MCPs
â”‚  â”‚  â”œâ”€ fusion.py                   # Result aggregation & provenance
â”‚  â”‚  â”œâ”€ schema_index.py             # Schema registry & search
â”‚  â”‚  â””â”€ mcp_manager.py              # MCP plugin lifecycle management
â”‚  â”œâ”€ models/
â”‚  â”‚  â””â”€ state.py                    # Pydantic models (PlanNode, ExecutionTask, LangGraphState)
â”‚  â”œâ”€ mcp_plugins/
â”‚  â”‚  â”œâ”€ mcp_sql_sample/             # SQL/PostgreSQL plugin entry
â”‚  â”‚  â”œâ”€ mcp_nosql_sample/           # NoSQL/MongoDB plugin entry
â”‚  â”‚  â”œâ”€ mcp_graph_sample/           # Graph/Neo4j plugin entry
â”‚  â”‚  â””â”€ mcp_vector_sample/          # Vector/Milvus plugin entry
â”‚  â””â”€ tests/
â”‚     â”œâ”€ test_unit_planner.py        # Planner logic tests
â”‚     â”œâ”€ test_unit_fusion.py         # Fusion logic tests
â”‚     â””â”€ test_end_to_end.py          # Full query pipeline tests
â”œâ”€ requirements.txt                  # Python dependencies
â”œâ”€ docker-compose.yml                # Multi-container orchestration (PostgreSQL, MongoDB, Neo4j, Milvus)
â”œâ”€ .env                              # Environment configuration
â”œâ”€ init-postgres/                    # PostgreSQL initialization scripts
â”œâ”€ init-mongo/                       # MongoDB initialization scripts
â”œâ”€ init-neo4j/                       # Neo4j initialization scripts
â””â”€ init-milvus/                      # Milvus initialization scripts
```

---

## ğŸ”Œ Core Components

### 1. **Planner** (`app/services/planner.py`)
- **Role**: Decomposes natural language queries into database-specific tasks
- **Technology**: Google Gemini 2.5 Flash LLM + deterministic fallback rules
- **Output**: List of `PlanNode` objects with:
  - Capability type (`query.sql`, `query.document`, `query.graph`, `query.vector`)
  - Natural language subquery
  - Preferred data source
  - Dependency graph (e.g., orders depend on customer lookup)

### 2. **Execution** (`app/services/execution.py`)
- **Role**: Orchestrates parallel execution of plan nodes across MCPs
- **Process**:
  1. Execute SQL node first (usually customer lookup)
  2. Use results as context for dependent nodes
  3. Run non-blocking queries in parallel (orders, referrals, similar customers)
  4. Collect results into `ExecutionTask` objects
- **Async**: Leverages `asyncio.gather()` for concurrency

### 3. **Fusion** (`app/services/fusion.py`)
- **Role**: Aggregates and enriches results from multiple sources
- **Provenance Tracking**: Each field includes metadata about its source
- **Output Structure**:
  ```json
  {
    "customer": {"id": "...", "name": {"value": "...", "provenance": [...]}},
    "recent_orders": [...],
    "referrals": [...],
    "similar_customers": [...],
    "explain": ["Customer info from sql_customers", "Orders from orders_mongo", ...]
  }
  ```

### 4. **Schema Index** (`app/services/schema_index.py`)
- **Role**: Maintains searchable registry of all database schemas
- **Indexed Artifacts**:
  - SQL tables & columns
  - MongoDB collections & fields
  - Neo4j node labels & properties
  - Milvus vector indices & metadata fields
- **Search**: Substring matching with relevance scoring

### 5. **MCP Manager** (`app/services/mcp_manager.py`)
- **Role**: Lifecycle management for Model Context Protocol plugins
- **Functions**: Register MCPs, route calls to correct plugin, handle failures

---

## ğŸ“¡ API Endpoints

### **POST `/api/v1/query`**
Execute a multi-database query.

**Request:**
```json
{
  "user_id": "user123",
  "nl_query": "Find all customers named John and their recent orders",
  "context": {}
}
```

**Response:**
```json
{
  "request_id": "550e8400-e29b-41d4-a716-446655440000",
  "status": "COMPLETE",
  "fused_data": {
    "customer": {
      "id": "cust_001",
      "name": {"value": "John Doe", "provenance": [{"source": "sql_customers", "field": "name"}]},
      "email": {"value": "john@example.com", "provenance": [{"source": "sql_customers", "field": "email"}]}
    },
    "recent_orders": [
      {"order_id": "ord_123", "amount": 99.99, "created_at": "2025-11-28T10:30:00Z"},
      {"order_id": "ord_124", "amount": 149.99, "created_at": "2025-11-27T14:15:00Z"}
    ],
    "referrals": [],
    "similar_customers": [],
    "explain": [
      "Customer info from sql_customers",
      "Orders from orders_mongo"
    ]
  }
}
```

### **GET `/api/v1/schema/search`**
Search for fields across all registered schemas.

**Query:**
```
GET /api/v1/schema/search?q=customer
```

**Response:**
```json
{
  "q": "customer",
  "hits": [
    {
      "id": "sql_customers.customers.id",
      "mcp": "sql_customers",
      "parent": "customers",
      "field": "id",
      "score": 1.0
    },
    {
      "id": "orders_mongo.orders.customer_id",
      "mcp": "orders_mongo",
      "parent": "orders",
      "field": "customer_id",
      "score": 1.0
    }
  ]
}
```

---

## ğŸš€ Getting Started

### Prerequisites

- **Python 3.10+**
- **Docker & Docker Compose** (for databases)
- **Google Gemini API Key** (optional; deterministic fallback available)

### Step 1: Clone & Install

```powershell
cd backend
python -m venv venv
.\venv\Scripts\Activate.ps1
python -m pip install --upgrade pip
pip install -r requirements.txt
```

### Step 2: Start Databases

```powershell
docker-compose up -d
```

This will start:
- **PostgreSQL** (5432) â€” customer master data
- **MongoDB** (27017) â€” order documents
- **Neo4j** (7687) â€” referral graph
- **Milvus** (19530) â€” customer embeddings

**Note:** Initial scripts in `init-*/` directories will populate sample data.

### Step 3: Configure Environment

Create or update `.env`:

```dotenv
# SQL (PostgreSQL)
POSTGRES_DSN=postgresql://postgres:postgrespassword@localhost:5432/mdbs

# NoSQL (MongoDB)
MONGO_URI=mongodb://localhost:27017
MONGO_DB=mdbs

# Graph (Neo4j)
NEO4J_URI=bolt://localhost:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=neo4jpassword

# Vector DB (Milvus)
MILVUS_HOST=localhost
MILVUS_PORT=19530

# LLM (Google Gemini)
GEMINI_API_KEY=your_api_key_here

# App
APP_ENV=development
```

### Step 4: Run the Backend

```powershell
python -m uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

Visit **http://localhost:8000/docs** (Swagger UI) or **http://localhost:8000/redoc** (ReDoc).

### Step 5: Run Tests

```powershell
# All tests
pytest -v

# Specific test file
pytest app/tests/test_end_to_end.py -v

# With coverage
pytest --cov=app app/tests/
```

---

## ğŸ’» Example Usage

### Via cURL

```powershell
$payload = @{
    user_id = "user_42"
    nl_query = "Show me customer John Smith and their orders"
    context = @{}
} | ConvertTo-Json

curl -X POST "http://localhost:8000/api/v1/query" `
  -Header "Content-Type: application/json" `
  -Body $payload
```

### Via Python

```python
import httpx
import asyncio

async def main():
    async with httpx.AsyncClient() as client:
        response = await client.post(
            "http://localhost:8000/api/v1/query",
            json={
                "user_id": "user_42",
                "nl_query": "Find customer John Smith and their recent orders",
                "context": {}
            }
        )
        print(response.json())

asyncio.run(main())
```

---

## ğŸ§ª Testing

### Unit Tests

**Planner Tests** (`app/tests/test_unit_planner.py`):
```python
pytest app/tests/test_unit_planner.py -v
```

**Fusion Tests** (`app/tests/test_unit_fusion.py`):
```python
pytest app/tests/test_unit_fusion.py -v
```

### End-to-End Tests

**Full Query Pipeline** (`app/tests/test_end_to_end.py`):
```python
pytest app/tests/test_end_to_end.py -v
```

Uses FastAPI `TestClient` to validate the `/api/v1/query` endpoint.

---

## ğŸ” Security & Environment

### Environment Variables

All sensitive configuration is in `.env`. **Never commit `.env` to version control.**

- `POSTGRES_DSN` â€” PostgreSQL connection string
- `MONGO_URI` â€” MongoDB connection URI
- `NEO4J_URI`, `NEO4J_USER`, `NEO4J_PASSWORD` â€” Neo4j credentials
- `GEMINI_API_KEY` â€” Google Generative AI API key
- `APP_ENV` â€” `"development"` or `"production"`

### CORS

Currently configured to allow all origins for development:
```python
allow_origins=["*"],
allow_methods=["*"],
allow_headers=["*"],
```

**âš ï¸ For production, restrict to specific domains.**

---

## ğŸ“¦ Dependencies

| Category | Packages |
|----------|----------|
| **Web Framework** | FastAPI, Uvicorn, httpx |
| **Data Models** | Pydantic, Pydantic-Settings |
| **Database Drivers** | psycopg2-binary (PostgreSQL), pymongo (MongoDB), neo4j (Neo4j), motor (async MongoDB) |
| **Vector DB** | qdrant-client, sentence-transformers |
| **LLM** | langchain, langgraph, google-generativeai, langchain-google-genai |
| **Utilities** | python-dotenv, numpy, networkx, redis, tenacity |
| **Testing** | pytest, pytest-asyncio, pytest-cov |

See `requirements.txt` for exact versions.

---

## ğŸƒ Quick Start Checklist

- [ ] Python 3.10+ installed
- [ ] Cloned repository
- [ ] Created & activated virtual environment
- [ ] Installed dependencies (`pip install -r requirements.txt`)
- [ ] Docker & Docker Compose running
- [ ] Started databases (`docker-compose up -d`)
- [ ] Configured `.env` with valid database credentials & Gemini API key
- [ ] Started backend (`python -m uvicorn app.main:app --reload`)
- [ ] Verified docs at `http://localhost:8000/docs`
- [ ] Ran tests (`pytest -v`)

---

## ğŸ¤ Contributing

When adding new services or endpoints:

1. **Add API route** in `app/api/v1/`
2. **Add service logic** in `app/services/`
3. **Update models** in `app/models/state.py` if needed
4. **Write tests** in `app/tests/`
5. **Document** in this README

---

## ğŸ“ License & Attribution

This project is part of the EDAI program at [Your University].

---

## ğŸ†˜ Troubleshooting

### Database Connection Errors
```
Check that docker-compose services are running:
docker-compose ps

Verify environment variables in .env match docker-compose credentials.
```

### Gemini API Errors
```
If GEMINI_API_KEY is missing or invalid, the planner falls back to deterministic rules.
Set GEMINI_API_KEY in .env to enable LLM-assisted planning.
```

### Port Conflicts
```
If port 8000 is already in use:
python -m uvicorn app.main:app --reload --port 8001
```

### Test Failures
```
Ensure databases are running (docker-compose ps)
and .env is properly configured.

Run with increased verbosity:
pytest -vv --tb=short
```

---

## ğŸ“ Support

For issues or questions:
1. Check the troubleshooting section above
2. Review inline code comments
3. Run tests to validate setup
4. Consult the API documentation at `http://localhost:8000/docs`
